{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "invalid-album",
   "metadata": {},
   "source": [
    "## How often does ROCES outperform the state of the art without requiring all examples to be used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "legendary-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "from utils.simple_solution import SimpleSolution\n",
    "from utils.evaluator import Evaluator\n",
    "from utils.data import Data\n",
    "from ontolearn.knowledge_base import KnowledgeBase\n",
    "from owlapy.render import DLSyntaxObjectRenderer\n",
    "from roces import BaseConceptSynthesis\n",
    "from roces.synthesizer import ConceptSynthesizer\n",
    "from owlapy.parser import DLSyntaxParser\n",
    "from utils.dataset import DatasetNoLabel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "import numpy as np, time\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import copy\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-fourth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "worth-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_roces_vocabulary(data_train, data_test, kb, args):\n",
    "    def add_data_values(path):\n",
    "        print(\"\\n*** Finding relevant data values ***\")\n",
    "        values = set()\n",
    "        for ce, lp in data_train+data_test:\n",
    "            if '[' in ce:\n",
    "                for val in re.findall(\"\\[(.*?)\\]\", ce):\n",
    "                    values.add(val.split(' ')[-1])\n",
    "        print(\"*** Done! ***\\n\")\n",
    "        print(\"Added values: \", values)\n",
    "        print()\n",
    "        return list(values)\n",
    "    renderer = DLSyntaxObjectRenderer()\n",
    "    individuals = [ind.get_iri().as_str().split(\"/\")[-1] for ind in kb.individuals()]\n",
    "    atomic_concepts = list(kb.ontology().classes_in_signature())\n",
    "    atomic_concept_names = [renderer.render(a) for a in atomic_concepts]\n",
    "    role_names = [rel.get_iri().get_remainder() for rel in kb.ontology().object_properties_in_signature()] + \\\n",
    "                 [rel.get_iri().get_remainder() for rel in kb.ontology().data_properties_in_signature()]\n",
    "    vocab = atomic_concept_names + role_names + ['⊔', '⊓', '∃', '∀', '¬', '⊤', '⊥', '.', ' ', '(', ')',\\\n",
    "                                                '⁻', '≤', '≥', 'True', 'False', '{', '}', ':', '[', ']',\n",
    "                                                'double', 'integer', 'date', 'xsd']\n",
    "    quantified_restriction_values = [str(i) for i in range(1,12)]\n",
    "    data_values = add_data_values(args.knowledge_base_path)\n",
    "    vocab = vocab + data_values + quantified_restriction_values\n",
    "    vocab = sorted(set(vocab)) + ['PAD']\n",
    "    print(\"Vocabulary size: \", len(vocab))\n",
    "    num_examples = min(args.num_examples, kb.individuals_count()//2)\n",
    "    return vocab, num_examples\n",
    "\n",
    "\n",
    "def before_pad(arg):\n",
    "    arg_temp = []\n",
    "    for atm in arg:\n",
    "        if atm == 'PAD':\n",
    "            break\n",
    "        arg_temp.append(atm)\n",
    "    return arg_temp\n",
    "\n",
    "num_examples = 1000\n",
    "def collate_batch(batch):\n",
    "    pos_emb_list = []\n",
    "    neg_emb_list = []\n",
    "    target_labels = []\n",
    "    for pos_emb, neg_emb, label in batch:\n",
    "        if pos_emb.ndim != 2:\n",
    "            pos_emb = pos_emb.reshape(1, -1)\n",
    "        if neg_emb.ndim != 2:\n",
    "            neg_emb = neg_emb.reshape(1, -1)\n",
    "        pos_emb_list.append(pos_emb)\n",
    "        neg_emb_list.append(neg_emb)\n",
    "        target_labels.append(label)\n",
    "    pos_emb_list[0] = F.pad(pos_emb_list[0], (0, 0, 0, num_examples - pos_emb_list[0].shape[0]), \"constant\", 0)\n",
    "    pos_emb_list = pad_sequence(pos_emb_list, batch_first=True, padding_value=0)\n",
    "    neg_emb_list[0] = F.pad(neg_emb_list[0], (0, 0, 0, num_examples - neg_emb_list[0].shape[0]), \"constant\", 0)\n",
    "    neg_emb_list = pad_sequence(neg_emb_list, batch_first=True, padding_value=0)\n",
    "    target_labels = pad_sequence(target_labels, batch_first=True, padding_value=-100)\n",
    "    return pos_emb_list, neg_emb_list, target_labels\n",
    "\n",
    "def predict(kb, positives, negatives, models, embedding_models, args):\n",
    "    args.path_to_triples = f\"datasets/{kb}/Triples/\"\n",
    "    global num_examples\n",
    "    num_examples = models[0].num_examples\n",
    "    vocab = models[0].vocab\n",
    "    inv_vocab = models[0].inv_vocab\n",
    "    kb_embedding_data = Data(args)\n",
    "    k = max(len(positives), len(negatives))\n",
    "    Scores = []\n",
    "    test_dataset = DatasetNoLabel([(\"dummy_key\", {\"positive examples\": positives, \"negative examples\": negatives})], kb_embedding_data, k) #data, triples_data, k\n",
    "    for i, (model, embedding_model) in enumerate(zip(models, embedding_models)):\n",
    "        model = model.eval()\n",
    "        scores = []\n",
    "        test_dataset.load_embeddings(embedding_model.eval())\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, num_workers=args.num_workers, shuffle=False)\n",
    "        for x1, x2 in tqdm(test_dataloader):\n",
    "            _, sc = model(x1, x2)\n",
    "            scores.append(sc.detach()) \n",
    "        scores = torch.cat(scores, 0)\n",
    "        if i == 0:\n",
    "            cum_scores = scores\n",
    "        else:\n",
    "            cum_scores = cum_scores + scores\n",
    "    avg_scores = cum_scores / len(models)\n",
    "    pred_sequence = model.inv_vocab[avg_scores.argmax(1)]\n",
    "    return pred_sequence[0]\n",
    "\n",
    "\n",
    "def initialize_synthesizer(vocab, num_examples, num_inds, args):\n",
    "    args.num_inds = num_inds\n",
    "    roces = ConceptSynthesizer(vocab, num_examples, args)\n",
    "    roces.refresh()\n",
    "    return roces.model, roces.embedding_model\n",
    "\n",
    "def synthesize_class_expression(kb_name, vocab, positives, negatives, num_examples, num_inds, args):\n",
    "    args.knowledge_base_path = \"datasets/\" + f\"{kb_name}/{kb_name}.owl\"\n",
    "    embs = torch.load(f\"datasets/{kb_name}/Model_weights/SetTransformer_{args.kb_emb_model}_Emb_inducing_points32.pt\", map_location=torch.device(\"cpu\"))\n",
    "    setattr(args, 'num_entities', embs['emb_ent_real.weight'].shape[0])\n",
    "    setattr(args, 'num_relations', embs['emb_rel_real.weight'].shape[0])\n",
    "    models, embedding_models = [], []\n",
    "    for inds in num_inds:\n",
    "        model, embedding_model = initialize_synthesizer(vocab, num_examples, inds, args)\n",
    "        if args.sampling_strategy != 'uniform':\n",
    "            model.load_state_dict(torch.load(f\"datasets/{kb_name}/Model_weights/{args.kb_emb_model}_SetTransformer_inducing_points{inds}.pt\", map_location=torch.device(\"cpu\")))\n",
    "            embedding_model.load_state_dict(torch.load(f\"datasets/{kb_name}/Model_weights/SetTransformer_{args.kb_emb_model}_Emb_inducing_points{inds}.pt\", map_location=torch.device(\"cpu\")))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(f\"datasets/{kb_name}/Model_weights/{args.kb_emb_model}_SetTransformer_uniform_inducing_points{inds}.pt\", map_location=torch.device(\"cpu\")))\n",
    "            embedding_model.load_state_dict(torch.load(f\"datasets/{kb_name}/Model_weights/SetTransformer_{args.kb_emb_model}_Emb_uniform_inducing_points{inds}.pt\", map_location=torch.device(\"cpu\")))\n",
    "        models.append(model)\n",
    "        embedding_models.append(embedding_model)\n",
    "    return predict(kb_name, positives, negatives, models, embedding_models, args)\n",
    "\n",
    "def prepare_utilities_for_roces(kb_name, args):\n",
    "    kb = KnowledgeBase(path=f\"datasets/{kb_name}/{kb_name}.owl\")\n",
    "    with open(f\"datasets/{kb_name}/Test_data/Data.json\", \"r\") as file:\n",
    "        test_data = json.load(file)\n",
    "    with open(f\"datasets/{kb_name}/Train_data/Data.json\", \"r\") as file:\n",
    "        train_data = json.load(file)\n",
    "    vocab, num_examples = build_roces_vocabulary(train_data, test_data, kb, args)\n",
    "    namespace = list(kb.individuals())[0].get_iri().get_namespace()\n",
    "    print(\"KB namespace: \", namespace)\n",
    "    print()\n",
    "    simpleSolution = SimpleSolution(kb)\n",
    "    evaluator = Evaluator(kb)\n",
    "    dl_parser = DLSyntaxParser(namespace = namespace)\n",
    "    all_individuals = set(kb.individuals())\n",
    "    return kb, simpleSolution, evaluator, dl_parser, all_individuals, vocab\n",
    "\n",
    "def prepare_utilities_search_based(kb_name, args):\n",
    "    kb = KnowledgeBase(path=f\"datasets/{kb_name}/{kb_name}.owl\")\n",
    "    namespace = list(kb.individuals())[0].get_iri().get_namespace()\n",
    "    print(\"KB namespace: \", namespace)\n",
    "    print()\n",
    "    simpleSolution = SimpleSolution(kb)\n",
    "    evaluator = Evaluator(kb)\n",
    "    dl_parser = DLSyntaxParser(namespace = namespace)\n",
    "    all_individuals = set(kb.individuals())\n",
    "    return kb, simpleSolution, evaluator, dl_parser, all_individuals\n",
    "\n",
    "\n",
    "def predict_with_roces(kb_name, vocab, positives, negatives, dl_parser, simpleSolution, args):\n",
    "    ensemble_models = \"+\".join([\"SetTransformer_I32\", \"SetTransformer_I64\", \"SetTransformer_I128\"])\n",
    "    num_inds = [int(model_name.split(\"I\")[-1]) for model_name in ensemble_models.split(\"+\")]\n",
    "    pred = synthesize_class_expression(kb_name, vocab, positives, negatives, num_examples, num_inds, args)\n",
    "    prediction = None\n",
    "    try:\n",
    "        end_idx = np.where(pred == 'PAD')[0][0] # remove padding token\n",
    "    except IndexError:\n",
    "        end_idx = -1\n",
    "    pred = pred[:end_idx]\n",
    "    try:\n",
    "        prediction = dl_parser.parse(\"\".join(pred.tolist()))\n",
    "    except Exception as err:\n",
    "        try:\n",
    "            pred = simpleSolution.predict(pred.sum())\n",
    "            prediction = dl_parser.parse(pred)\n",
    "        except Exception:\n",
    "            print(f\"Could not understand expression {pred}\")\n",
    "    if prediction is None:\n",
    "        prediction = dl_parser.parse('⊤')\n",
    "    return prediction\n",
    "\n",
    "def query_oracle(prediction, oracle, kb, positives, negatives, all_individuals, pos_diff, neg_diff, remove_wrong_examples, subset_size):\n",
    "    if remove_wrong_examples:\n",
    "        if len(positives)-pos_diff >= subset_size:\n",
    "            positives = positives[:-pos_diff]\n",
    "        if len(negatives)-neg_diff >= subset_size:\n",
    "            negatives = negatives[:-neg_diff]\n",
    "    true_positive_examples = set([ind.get_iri().as_str().split(\"/\")[-1] for ind in kb.individuals(oracle)])\n",
    "    true_negative_examples = all_individuals-true_positive_examples\n",
    "    predicted_positives = set([ind.get_iri().as_str().split(\"/\")[-1] for ind in kb.individuals(prediction)])\n",
    "    covered_positives = predicted_positives.intersection(true_positive_examples)\n",
    "    candidate_negatives = true_negative_examples.intersection(all_individuals.difference(predicted_positives))\n",
    "    #num_neg_samples = min(len(positives), len(negatives), len(candidate_negatives))\n",
    "    candidate_positives = true_positive_examples.intersection(covered_positives)\n",
    "    if not candidate_positives:\n",
    "        candidate_positives = all_individuals.difference(set(positives).union(set(negatives)))\n",
    "    if not candidate_negatives:\n",
    "        candidate_negatives = all_individuals.difference(set(negatives).union(set(positives)))\n",
    "    new_positives = positives + random.sample(list(candidate_positives), min(subset_size, len(candidate_positives)))\n",
    "    new_negatives = negatives + random.sample(list(candidate_negatives), min(subset_size, len(candidate_negatives)))\n",
    "    if len(new_positives) < subset_size:\n",
    "        new_positives = new_positives + list(candidate_positives)[:subset_size-1]\n",
    "    if len(new_negatives) < subset_size:\n",
    "        new_negatives = new_negatives + list(candidate_negatives)[:subset_size-1]\n",
    "    #num_pos_samples = min(len(positives), len(negatives), len(candidate_positives))\n",
    "    return new_positives, new_negatives\n",
    "    \n",
    "def evaluate_prediction(kb, prediction, oracle, evaluator, simpleSolution, all_individuals):\n",
    "    positive_examples = set(kb.individuals(oracle))\n",
    "    negative_examples = all_individuals-positive_examples\n",
    "    try:\n",
    "        _, f1 = evaluator.evaluate(prediction, positive_examples, negative_examples)\n",
    "    except Exception as err:\n",
    "        print(f\"Parsing error on \", prediction)\n",
    "        prediction = dl_parser.parse('⊤')\n",
    "        _, f1 = evaluator.evaluate(prediction, positive_examples, negative_examples)\n",
    "    try:\n",
    "        prediction_str = simpleSolution.renderer.render(prediction)\n",
    "    except:\n",
    "        prediction_str = \"Unknown\"\n",
    "    return prediction_str, f1\n",
    "\n",
    "def start_active_learning(kb_name, oracle, positives, negatives, args, max_iter=10, subset_size=5, approach=\"roces\"):\n",
    "    ###### Improve here\n",
    "    i = 0\n",
    "    best_prediction = None\n",
    "    best_score = None\n",
    "    previous_score = None\n",
    "    all_predictions = []\n",
    "    F1 = []\n",
    "    if approach == \"roces\":\n",
    "        kb, simpleSolution, evaluator, dl_parser, all_individuals, vocab = prepare_utilities_for_roces(kb_name, args)\n",
    "    else:\n",
    "        kb, simpleSolution, evaluator, dl_parser, all_individuals = prepare_utilities_search_based(kb_name, args)\n",
    "    all_individuals_str = set([ind.get_iri().as_str().split(\"/\")[-1] for ind in all_individuals])\n",
    "    oracle = dl_parser.parse(oracle)\n",
    "    while i < max_iter:\n",
    "        if approach == \"roces\":\n",
    "            if i == 0:\n",
    "                prediction = predict_with_roces(kb_name, vocab, positives, negatives, dl_parser, simpleSolution, args)\n",
    "                prediction_str, f1 = evaluate_prediction(kb, prediction, oracle, evaluator, simpleSolution, all_individuals)\n",
    "                best_prediction = prediction_str\n",
    "                all_predictions.append(prediction_str)\n",
    "                F1.append(f1)\n",
    "                new_positives, new_negatives = positives, negatives\n",
    "                pos_diff = len(new_positives) - len(positives)\n",
    "                neg_diff = len(new_negatives) - len(negatives)\n",
    "                previous_score = f1\n",
    "                best_score = f1\n",
    "            else:\n",
    "                remove_wrong_examples = (previous_score > f1 or f1==0)\n",
    "                copy_pos = new_positives\n",
    "                copy_neg = new_negatives\n",
    "                new_positives, new_negatives = query_oracle(prediction, oracle, kb, new_positives, new_negatives, all_individuals_str, pos_diff, neg_diff, remove_wrong_examples, subset_size)\n",
    "                pos_diff = len(new_positives) - len(copy_pos)\n",
    "                neg_diff = len(new_negatives) - len(copy_neg)\n",
    "                print(\"new positives\", len(new_positives))\n",
    "                print(\"new negatives\", len(new_negatives))\n",
    "                #print(\"pos diff\", pos_diff)\n",
    "                #print(\"neg diff\", neg_diff)\n",
    "                prediction = predict_with_roces(kb_name, vocab, new_positives, new_negatives, dl_parser, simpleSolution, args)\n",
    "                previous_score = f1\n",
    "                prediction_str, f1 = evaluate_prediction(kb, prediction, oracle, evaluator, simpleSolution, all_individuals)\n",
    "                all_predictions.append(prediction_str)\n",
    "                F1.append(f1)\n",
    "                if f1 > previous_score:\n",
    "                    best_prediction = prediction_str\n",
    "        else:\n",
    "            pass\n",
    "        if f1 > best_score:\n",
    "            print(\"improved performance\")\n",
    "            best_score = f1\n",
    "        i += 1\n",
    "        print()\n",
    "        if f1 == 100:\n",
    "            break\n",
    "    return best_prediction, all_predictions, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coordinate-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\") as config:\n",
    "    nces_args = json.load(config)\n",
    "    nces_args = Namespace(**nces_args)\n",
    "nces_args.kb_emb_model = \"ConEx\"\n",
    "nces_args.sampling_strategy = \"original\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stone-citizenship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CognitiveAgent ⊔ LandArea\n",
      "Agent ⊓ (Human ⊔ (¬God)) ⊓ (∀ spouseOf.Woman)\n",
      "Series ⊔ (∃ relativeOf.(Woman ⊓ (∀ visitedPlace.⊥)))\n",
      "(LandArea ⊓ (¬City)) ⊔ (∃ member.Agent)\n",
      "(Mountain ⊓ (∀ location.⊥)) ⊔ (∃ knows.(¬SonOfGod))\n"
     ]
    }
   ],
   "source": [
    "kb_name = \"semantic_bible\"\n",
    "with open(f\"datasets/{kb_name}/Test_data/Data.json\") as file:\n",
    "    test_data = json.load(file)\n",
    "    test_lps = [lp for lp,examples in test_data]\n",
    "    full_examples = [examples for lp,examples in test_data]\n",
    "    print(\"\\n\".join(test_lps[:5]))\n",
    "oracle = test_lps[0]\n",
    "full_positives, full_negatives = full_examples[0][\"positive examples\"], full_examples[0][\"negative examples\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "centered-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = random.sample(full_positives, 5)\n",
    "negatives = random.sample(full_negatives, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "stopped-translation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CognitiveAgent ⊔ LandArea'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "blessed-insert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NTNames#Nain',\n",
       " 'NTNames#JosephSonOfMattathias',\n",
       " 'NTNames#Zebulun',\n",
       " 'NTNames#Olivet',\n",
       " 'NTNames#Alexander']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "civic-costs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NTNames#NeapolisGeodata',\n",
       " 'NTNames#Zealot',\n",
       " 'NTNames#PhoenixGeodata',\n",
       " 'NTNames#Ephesians',\n",
       " 'NTNames#SycharGeodata']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "governing-overview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Finding relevant data values ***\n",
      "*** Done! ***\n",
      "\n",
      "Added values:  {'35.583', '41.013', '44.42'}\n",
      "\n",
      "Vocabulary size:  126\n",
      "KB namespace:  http://semanticbible.org/ns/2006/NTNames#\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 15\n",
      "new negatives 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 25\n",
      "new negatives 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improved performance\n",
      "\n",
      "new positives 35\n",
      "new negatives 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 45\n",
      "new negatives 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 45\n",
      "new negatives 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improved performance\n",
      "\n",
      "new positives 55\n",
      "new negatives 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 55\n",
      "new negatives 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 65\n",
      "new negatives 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 65\n",
      "new negatives 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 75\n",
      "new negatives 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 85\n",
      "new negatives 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 95\n",
      "new negatives 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 105\n",
      "new negatives 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 115\n",
      "new negatives 115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 125\n",
      "new negatives 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 135\n",
      "new negatives 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 145\n",
      "new negatives 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 155\n",
      "new negatives 155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 165\n",
      "new negatives 165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 175\n",
      "new negatives 175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 185\n",
      "new negatives 185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 195\n",
      "new negatives 195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 205\n",
      "new negatives 205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 215\n",
      "new negatives 215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 225\n",
      "new negatives 225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 235\n",
      "new negatives 235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 245\n",
      "new negatives 245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 255\n",
      "new negatives 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 265\n",
      "new negatives 265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 275\n",
      "new negatives 275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 285\n",
      "new negatives 285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 295\n",
      "new negatives 295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 305\n",
      "new negatives 305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 315\n",
      "new negatives 315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 325\n",
      "new negatives 325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 335\n",
      "new negatives 335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 345\n",
      "new negatives 345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 355\n",
      "new negatives 355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 365\n",
      "new negatives 365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 375\n",
      "new negatives 375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 385\n",
      "new negatives 385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 395\n",
      "new negatives 395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 405\n",
      "new negatives 405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 415\n",
      "new negatives 415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 425\n",
      "new negatives 425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.23it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 435\n",
      "new negatives 435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 445\n",
      "new negatives 445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 455\n",
      "new negatives 455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new positives 465\n",
      "new negatives 465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_prediction, all_predictions, F1 = start_active_learning(kb_name, oracle, positives, negatives, nces_args, max_iter=50, subset_size=2*len(positives), approach=\"roces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "alone-tonight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 35.881,\n",
       " 35.881,\n",
       " 0.0,\n",
       " 84.962,\n",
       " 35.881,\n",
       " 35.881,\n",
       " 0.0,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962,\n",
       " 84.962]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "productive-cologne",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CitizenshipAttribute',\n",
       " 'Mountain',\n",
       " 'FixedHoliday',\n",
       " 'FixedHoliday ⊔ (∃ location.Woman)',\n",
       " 'SonOfGod',\n",
       " 'EthnicGroup',\n",
       " '⊤',\n",
       " '⊤',\n",
       " '⊤',\n",
       " '⊤',\n",
       " '⊤',\n",
       " '⊤',\n",
       " '⊤',\n",
       " '⊤',\n",
       " '⊤',\n",
       " '⊤',\n",
       " 'CognitiveAgent ⊔ BeliefGroup',\n",
       " 'CognitiveAgent ⊔ BeliefGroup',\n",
       " 'CognitiveAgent ⊔ BeliefGroup',\n",
       " 'CognitiveAgent',\n",
       " '⊤',\n",
       " '⊤',\n",
       " '⊤',\n",
       " '⊤',\n",
       " '⊤',\n",
       " 'CognitiveAgent',\n",
       " '⊤',\n",
       " 'CognitiveAgent ⊔ BeliefGroup',\n",
       " 'CognitiveAgent ⊔ BeliefGroup',\n",
       " 'CognitiveAgent ⊔ BeliefGroup']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "connected-basics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CognitiveAgent ⊔ LandArea'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "behavioral-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./datasets/carcinogenesis/Train_data/Data.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "expired-wichita",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "≤ 9 hasAtom.⊤\n",
      "≤ 9 hasAtom.⊤\n",
      "≤ 9 hasStructure.⊤\n",
      "≤ 9 hasStructure.⊤\n",
      "Compound ⊓ (≤ 9 hasBond.⊤)\n",
      "Compound ⊓ (≤ 9 hasBond.⊤)\n",
      "≤ 9 hasBond.⊤\n",
      "≤ 9 hasBond.⊤\n"
     ]
    }
   ],
   "source": [
    "for ce, example in data:\n",
    "    if \"≤\" in ce:\n",
    "        print(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-running",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roces",
   "language": "python",
   "name": "roces"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
